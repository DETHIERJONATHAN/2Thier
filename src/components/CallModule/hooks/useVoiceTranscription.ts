/**
 * üéôÔ∏è useVoiceTranscription - Hook pour la transcription vocale en temps r√©el
 * 
 * Fonctionnalit√©s avanc√©es :
 * - üé§ Transcription live pendant l'appel
 * - üó£Ô∏è D√©tection du locuteur (agent/prospect)
 * - üß† Analyse sentiment temps r√©el
 * - üìù G√©n√©ration automatique de notes
 * - üéØ D√©tection de mots-cl√©s importants
 * - üîÑ Synchronisation avec l'IA assistant
 */

import { useState, useCallback, useEffect, useMemo, useRef } from 'react';
import { useAuthenticatedApi } from '../../../hooks/useAuthenticatedApi';
import { NotificationManager } from '../../../components/Notifications';
import type { 
  UseVoiceTranscriptionReturn,
  VoiceTranscription,
  TranscriptionState,
  VoiceAnalysis,
  SpeakerDetection,
  KeywordDetection,
  Lead
} from '../types/CallTypes';

// Types pour Speech Recognition API
declare global {
  interface SpeechRecognition extends EventTarget {
    continuous: boolean;
    interimResults: boolean;
    lang: string;
    maxAlternatives: number;
    start(): void;
    stop(): void;
    onstart: ((this: SpeechRecognition, ev: Event) => void) | null;
    onresult: ((this: SpeechRecognition, ev: SpeechRecognitionEvent) => void) | null;
    onerror: ((this: SpeechRecognition, ev: SpeechRecognitionErrorEvent) => void) | null;
    onend: ((this: SpeechRecognition, ev: Event) => void) | null;
  }

  interface SpeechRecognitionEvent extends Event {
    resultIndex: number;
    results: SpeechRecognitionResultList;
  }

  interface SpeechRecognitionErrorEvent extends Event {
    error: string;
    message: string;
  }

  interface SpeechRecognitionResultList {
    length: number;
    item(index: number): SpeechRecognitionResult;
    [index: number]: SpeechRecognitionResult;
  }

  interface SpeechRecognitionResult {
    length: number;
    isFinal: boolean;
    item(index: number): SpeechRecognitionAlternative;
    [index: number]: SpeechRecognitionAlternative;
  }

  interface SpeechRecognitionAlternative {
    transcript: string;
    confidence: number;
  }

  var SpeechRecognition: {
    prototype: SpeechRecognition;
    new(): SpeechRecognition;
  };

  var webkitSpeechRecognition: {
    prototype: SpeechRecognition;
    new(): SpeechRecognition;
  };
}

// Type pour le stream avec timer
interface MediaStreamWithTimer extends MediaStream {
  timer?: NodeJS.Timeout;
}

export const useVoiceTranscription = (
  leadId: string,
  lead: Lead | null,
  callInProgress: boolean = false,
  onTranscriptionUpdate?: (transcription: VoiceTranscription) => void
): UseVoiceTranscriptionReturn => {
  
  const { api } = useAuthenticatedApi();
  const streamRef = useRef<MediaStreamWithTimer | null>(null);
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  
  // üéôÔ∏è √âtat principal de la transcription
  const [transcriptionState, setTranscriptionState] = useState<TranscriptionState>({
    isActive: false,
    isListening: false,
    currentText: '',
    transcriptionHistory: [],
    currentSpeaker: null,
    confidence: 0,
    wordCount: 0,
    duration: 0
  });
  
  const [voiceAnalysis, setVoiceAnalysis] = useState<VoiceAnalysis | null>(null);
  const [keywordDetections, setKeywordDetections] = useState<KeywordDetection[]>([]);
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState<string>();
  
  // üó£Ô∏è D√©tection basique du locuteur (√† am√©liorer avec IA)
  const detectSpeaker = useCallback((transcript: string): SpeakerDetection => {
    // Mots-cl√©s agent commercial
    const agentKeywords = [
      'bonjour', 'madame', 'monsieur', 'soci√©t√©', 'proposition', 'offre',
      'rendez-vous', 'rdv', 'pr√©senter', 'expliquer', 'service', 'solution'
    ];
    
    // Mots-cl√©s prospect
    const prospectKeywords = [
      'int√©ress√©', 'prix', 'tarif', 'budget', 'r√©fl√©chir', 'coll√®gues',
      'd√©cision', 'patron', 'd√©j√†', 'actuellement', 'satisfait'
    ];
    
    const lowerTranscript = transcript.toLowerCase();
    const agentScore = agentKeywords.filter(word => lowerTranscript.includes(word)).length;
    const prospectScore = prospectKeywords.filter(word => lowerTranscript.includes(word)).length;
    
    if (agentScore > prospectScore) {
      return {
        type: 'agent',
        confidence: Math.min(0.9, 0.5 + (agentScore * 0.1))
      };
    } else if (prospectScore > agentScore) {
      return {
        type: 'prospect',
        confidence: Math.min(0.9, 0.5 + (prospectScore * 0.1))
      };
    } else {
      return {
        type: 'unknown',
        confidence: 0.3
      };
    }
  }, []);
  
  // üß† Analyser le contenu avec l'IA
  const analyzeTranscriptContent = useCallback(async (
    transcription: VoiceTranscription
  ): Promise<void> => {
    try {
      const response = await api.post<{
        analysis: VoiceAnalysis;
        keywords: KeywordDetection[];
        sentiment: 'positive' | 'neutral' | 'negative';
        recommendations: string[];
      }>('/api/ai/analyze-voice-content', {
        transcription,
        leadId: leadId,
        leadData: lead?.data,
        conversationContext: transcriptionState.transcriptionHistory.slice(-5)
      });
      
      if (response.analysis) {
        setVoiceAnalysis(response.analysis);
        
        // Ajouter les nouveaux mots-cl√©s d√©tect√©s
        if (response.keywords) {
          setKeywordDetections(prev => [...prev, ...response.keywords]);
        }
        
        console.log('[useVoiceTranscription] üß† Analyse IA termin√©e - Sentiment:', response.sentiment);
      }
      
    } catch (error: unknown) {
      console.warn('[useVoiceTranscription] ‚ö†Ô∏è Erreur analyse contenu:', error);
    }
  }, [api, leadId, lead, transcriptionState.transcriptionHistory]);
  
  // üìù Traiter une transcription finale
  const processFinalTranscription = useCallback(async (
    transcript: string,
    confidence: number
  ): Promise<void> => {
    if (transcript.trim().length < 3) return;
    
    const transcriptionEntry: VoiceTranscription = {
      id: `transcription_${Date.now()}`,
      text: transcript.trim(),
      timestamp: new Date(),
      speaker: detectSpeaker(transcript), // D√©tection basique du locuteur
      confidence: confidence || 0.8,
      duration: 0, // Calcul√© c√¥t√© serveur
      language: 'fr-FR'
    };
    
    // Ajouter √† l'historique
    setTranscriptionState(prev => ({
      ...prev,
      transcriptionHistory: [...prev.transcriptionHistory, transcriptionEntry],
      currentText: '',
      wordCount: prev.wordCount + transcript.split(' ').length
    }));
    
    // Notifier le parent
    onTranscriptionUpdate?.(transcriptionEntry);
    
    // Analyser le contenu avec l'IA
    try {
      await analyzeTranscriptContent(transcriptionEntry);
    } catch (error) {
      console.warn('[useVoiceTranscription] ‚ö†Ô∏è Erreur analyse IA:', error);
    }
  }, [onTranscriptionUpdate, analyzeTranscriptContent, detectSpeaker]);
  
  // üé§ Initialiser la reconnaissance vocale Web Speech API
  const initializeSpeechRecognition = useCallback((): boolean => {
    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
      setError('Reconnaissance vocale non support√©e par ce navigateur');
      NotificationManager.error('Reconnaissance vocale non support√©e');
      return false;
    }
    
    try {
      const SpeechRecognition = (window as { webkitSpeechRecognition?: unknown; SpeechRecognition?: unknown })
        .webkitSpeechRecognition || (window as { SpeechRecognition?: unknown }).SpeechRecognition;
      const recognition = new SpeechRecognition();
      
      // Configuration de base
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'fr-FR';
      recognition.maxAlternatives = 3;
      
      // √âv√©nements de reconnaissance
      recognition.onstart = () => {
        console.log('[useVoiceTranscription] üé§ Reconnaissance vocale d√©marr√©e');
        setTranscriptionState(prev => ({ ...prev, isListening: true }));
      };
      
      recognition.onresult = (event: SpeechRecognitionEvent) => {
        let interimTranscript = '';
        
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          const confidence = event.results[i][0].confidence;
          
          if (event.results[i].isFinal) {
            // Traiter la transcription finale
            processFinalTranscription(transcript, confidence);
          } else {
            interimTranscript += transcript;
          }
        }
        
        // Mettre √† jour le texte courant
        setTranscriptionState(prev => ({
          ...prev,
          currentText: interimTranscript,
          confidence: event.results[event.results.length - 1]?.[0]?.confidence || 0
        }));
      };
      
      recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
        console.error('[useVoiceTranscription] ‚ùå Erreur reconnaissance:', event.error);
        if (event.error === 'no-speech') {
          // Pas de probl√®me, pas de parole d√©tect√©e
          return;
        }
        setError(`Erreur reconnaissance vocale: ${event.error}`);
      };
      
      recognition.onend = () => {
        console.log('[useVoiceTranscription] üîö Reconnaissance vocale termin√©e');
        setTranscriptionState(prev => ({ ...prev, isListening: false }));
        
        // Red√©marrer automatiquement si toujours actif
        if (transcriptionState.isActive && callInProgress) {
          setTimeout(() => {
            recognition.start();
          }, 100);
        }
      };
      
      recognitionRef.current = recognition;
      return true;
      
    } catch (error: unknown) {
      console.error('[useVoiceTranscription] ‚ùå Erreur initialisation:', error);
      setError('Impossible d\'initialiser la reconnaissance vocale');
      return false;
    }
  }, [transcriptionState.isActive, callInProgress, processFinalTranscription]);
  
  // üé§ D√©marrer la transcription
  const startTranscription = useCallback(async (): Promise<void> => {
    if (!callInProgress) {
      NotificationManager.warning('D√©marrez un appel pour activer la transcription');
      return;
    }
    
    setIsLoading(true);
    setError(undefined);
    
    try {
      // Demander permission microphone
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        } 
      });
      
      streamRef.current = stream;
      
      // Initialiser la reconnaissance vocale
      if (initializeSpeechRecognition()) {
        recognitionRef.current?.start();
        
        setTranscriptionState(prev => ({
          ...prev,
          isActive: true,
          transcriptionHistory: [],
          wordCount: 0,
          duration: 0
        }));
        
        // D√©marrer le timer
        const startTime = Date.now();
        const timer = setInterval(() => {
          setTranscriptionState(prev => ({
            ...prev,
            duration: Math.floor((Date.now() - startTime) / 1000)
          }));
        }, 1000);
        
        // Sauvegarder le timer pour le cleanup
        if (streamRef.current) {
          streamRef.current.timer = timer;
        }
        
        console.log('[useVoiceTranscription] ‚úÖ Transcription d√©marr√©e');
        NotificationManager.success('üé§ Transcription vocale activ√©e');
      }
      
    } catch (error: unknown) {
      console.error('[useVoiceTranscription] ‚ùå Erreur d√©marrage:', error);
      const errorMessage = error instanceof Error ? error.message : 'Erreur d\'acc√®s au microphone';
      setError(errorMessage);
      NotificationManager.error('Impossible d\'acc√©der au microphone');
      
    } finally {
      setIsLoading(false);
    }
  }, [callInProgress, initializeSpeechRecognition]);
  
  // üõë Arr√™ter la transcription
  const stopTranscription = useCallback((): void => {
    try {
      // Arr√™ter la reconnaissance
      if (recognitionRef.current) {
        recognitionRef.current.stop();
        recognitionRef.current = null;
      }
      
      // Arr√™ter le stream microphone
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
        
        // Arr√™ter le timer
        if (streamRef.current?.timer) {
          clearInterval(streamRef.current.timer);
        }
        
        streamRef.current = null;
      }
      
      setTranscriptionState(prev => ({
        ...prev,
        isActive: false,
        isListening: false,
        currentText: ''
      }));
      
      console.log('[useVoiceTranscription] üõë Transcription arr√™t√©e');
      NotificationManager.info('Transcription vocale arr√™t√©e');
      
    } catch (error: unknown) {
      console.error('[useVoiceTranscription] ‚ùå Erreur arr√™t:', error);
    }
  }, []);
  
  // üîÑ Toggle transcription
  const toggleTranscription = useCallback((): void => {
    if (transcriptionState.isActive) {
      stopTranscription();
    } else {
      startTranscription();
    }
  }, [transcriptionState.isActive, stopTranscription, startTranscription]);
  
  // üìä Statistiques de transcription
  const transcriptionStats = useMemo(() => {
    const totalTranscriptions = transcriptionState.transcriptionHistory.length;
    const agentMessages = transcriptionState.transcriptionHistory.filter(
      t => t.speaker?.type === 'agent'
    ).length;
    const prospectMessages = transcriptionState.transcriptionHistory.filter(
      t => t.speaker?.type === 'prospect'
    ).length;
    
    const avgConfidence = transcriptionState.transcriptionHistory.length > 0
      ? transcriptionState.transcriptionHistory.reduce((acc, t) => acc + t.confidence, 0) / totalTranscriptions
      : 0;
    
    return {
      totalTranscriptions,
      agentMessages,
      prospectMessages,
      avgConfidence: Math.round(avgConfidence * 100),
      wordCount: transcriptionState.wordCount,
      duration: transcriptionState.duration,
      keywordsDetected: keywordDetections.length
    };
  }, [transcriptionState, keywordDetections]);
  
  // üßπ Cleanup lors du d√©montage
  useEffect(() => {
    return () => {
      if (transcriptionState.isActive) {
        stopTranscription();
      }
    };
  }, [transcriptionState.isActive, stopTranscription]);
  
  // üîÑ Auto-arr√™t si l'appel se termine
  useEffect(() => {
    if (!callInProgress && transcriptionState.isActive) {
      stopTranscription();
    }
  }, [callInProgress, transcriptionState.isActive, stopTranscription]);
  
  // üéØ Retour du hook avec toutes les fonctionnalit√©s
  const returnValue: UseVoiceTranscriptionReturn = useMemo(() => ({
    transcriptionState,
    voiceAnalysis,
    keywordDetections,
    startTranscription,
    stopTranscription,
    toggleTranscription,
    transcriptionStats,
    isLoading,
    error
  }), [
    transcriptionState,
    voiceAnalysis,
    keywordDetections,
    startTranscription,
    stopTranscription,
    toggleTranscription,
    transcriptionStats,
    isLoading,
    error
  ]);
  
  return returnValue;
};

export default useVoiceTranscription;
